<!DOCTYPE html>
<html>
  <!-- Html Head Tag-->
  <head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="zesen.he">
  
  
  <!-- Open Graph Data -->
  <meta property="og:title" content="贴吧简单爬虫需求及可视化"/>
  <meta property="og:description" content="少女情怀总是湿" />
  <meta property="og:site_name" content="小森森的个人博客"/>
  <meta property="og:type" content="article" />
  <meta property="og:image" content="http://yoursite.com"/>
  
    <link rel="alternate" href="/atom.xml" title="小森森的个人博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/favicon.ico">
  

  <!-- Site Title -->
  <title>小森森的个人博客</title>

  <!-- Bootstrap CSS -->
  
<link rel="stylesheet" href="/css/bootstrap.min.css">

  
<link rel="stylesheet" href="/css/font-awesome.min.css">

  <!-- Custom CSS -->
  
<link rel="stylesheet" href="/css/style.css">


  <!-- Google Analytics -->
  

  <!-- BaiDu Analytics -->
  



<meta name="generator" content="Hexo 4.2.0"></head>

  <body>
    <!-- Page Header -->


<nav class="navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse site-menu">
            <ul class="nav navbar-nav navbar-right">
                
                    <li>
                        <a href="/">
                            
                                Home
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/archives">
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/about">
                            
                                About
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="mailto:hezesen@qq.com">
                            
                                Email
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/%5Bobject%20Object%5D">
                            
                                <i class="icon fa fa-github fa-lg"></i>
                            
                        </a>
                    </li>
                
            </ul>
        </div>
    </div>
</nav>

<!-- Header -->
<header class="site-header header-background intro-header" style="background-image: url(/img/default-banner.jpg)">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="page-title with-background-image">
          <p class="title">贴吧简单爬虫需求及可视化</p>
          <p class="subtitle"></p>
        </div>
      </div>
    </div>
  </div>
</header>

<article>
  <div class="container typo">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-info text-muted">
          
            <!-- Author -->
            <span class="author info">By zesen.he</span>
          
          <!-- Date -->
          <span class="date-time info">On
            <span class="date">2020-03-23T14:12:40+08:00</span>
            <!--<span class="date">2020-03-23</span>
            <span class="time">10:12:31</span> -->
          </span>
          
        </div>
        <!-- Tags -->
        
        <!-- Post Main Content -->
        <div class="post-content">
          <blockquote>
<p> 最近有一些 tieba 以及 douban 的一些爬虫需求，所以记录一下。</p>
</blockquote>
<p>爬虫部分主要依赖 request，以及 bs4 解析，部分非格式化的用了正则捞取。</p>
<p>由于需求中提到了发帖者的吧龄，性别还有发帖数量等信息，需要从贴吧首页请求到楼主的主页，获取信息，所以解析楼主主页的链接格式花了些时间，在此感谢囧哥 @zqj 。</p>
<p>代理开了 global mode，且请求时间较久，截止目前已经连续跑了两天左右无异常。</p>
<p>可视化部分主要是针对 标题和发帖内容做聚合后分词，做词频统计，并进行词云可视化，这里以标题为例，发帖内容逻辑一致。</p>
<h3 id="爬虫部分"><a href="#爬虫部分" class="headerlink" title="爬虫部分"></a>爬虫部分</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2020-03-20 15:21</span></span><br><span class="line"><span class="comment"># @Author  : zesen.he</span></span><br><span class="line"><span class="comment"># @File    : tiebaRequest.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># sys.stdout = io.TextIOWrapper(sys.stdout.buffer,encoding='gbk') #改变标准输出的默认编码</span></span><br><span class="line"><span class="comment"># 中国传媒大学</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"> # 标题&amp;帖子链接：</span></span><br><span class="line"><span class="string">    &lt;a rel="noreferrer" href="/p/4788526595" title="我的人物设计和制作" target="_blank" class="j_th_tit "&gt;我的人物设计和制作&lt;/a&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#发帖人：</span></span><br><span class="line"><span class="string">    &lt;span class="tb_icon_author " title="主题作者: 新日落" data-field="&#123;"user_id":2137596235&#125;"&gt;&lt;i class="icon_author"&gt;&lt;/i&gt;&lt;span class="frs-author-name-wrap"&gt;&lt;a rel="noreferrer" data-field="&#123;"un":"\u65b0\u65e5\u843d"&#125;" class="frs-author-name j_user_card " href="/home/main/?un=%E6%96%B0%E6%97%A5%E8%90%BD&amp;ie=utf-8&amp;fr=frs" target="_blank"&gt;新日落&lt;/a&gt;&lt;/span&gt;&lt;span class="icon_wrap  icon_wrap_theme1 frs_bright_icons "&gt;&lt;/span&gt;    &lt;/span&gt;</span></span><br><span class="line"><span class="string">#发帖日期：</span></span><br><span class="line"><span class="string">  &lt;span class="pull-right is_show_create_time" title="创建时间"&gt;2016-09&lt;/span&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#回复数量：</span></span><br><span class="line"><span class="string">    &lt;div class="col2_left j_threadlist_li_left"&gt;</span></span><br><span class="line"><span class="string">&lt;span class="threadlist_rep_num center_text" title="回复"&gt;73&lt;/span&gt;</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 抓取网页的通用框架,获取页面的内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHtml</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url, timeout=<span class="number">30</span>)</span><br><span class="line">        <span class="comment"># 状态码不是200就发出httpError的异常</span></span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        <span class="comment"># 获取正确的编码格式</span></span><br><span class="line">        <span class="comment"># r.encoding=r.apparent_encoding</span></span><br><span class="line">        r.encoding = <span class="string">"utf-8"</span></span><br><span class="line">        <span class="comment"># 打印内容</span></span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"request wrong!"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分析网页的html文件，整理信息，保存问列表文件中</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_content</span><span class="params">(url,tiebaName)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化一个列表来保存所有的帖子信息</span></span><br><span class="line">    print(<span class="string">'目前正在爬取&#123;&#125;'</span>.format(tiebaName))</span><br><span class="line"></span><br><span class="line">    contents = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取网页的内容</span></span><br><span class="line">    html = getHtml(url)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将网页内容格式化利用bs4库</span></span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取所有的li标签属性为 j_thread_list clearfix，用列表接收</span></span><br><span class="line">    liTags = soup.find_all(<span class="string">'li'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'j_thread_list clearfix'</span>&#125;)</span><br><span class="line">    print(len(liTags))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 循环这个内容li集合</span></span><br><span class="line">    <span class="keyword">for</span> li <span class="keyword">in</span> liTags:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将爬取到了每一条信息。保存到字典里</span></span><br><span class="line">        content = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将异样抛出，避免无数据时，停止运行</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># 开始筛选信息</span></span><br><span class="line">            content[<span class="string">'tiebaName'</span>] = tiebaName</span><br><span class="line"></span><br><span class="line">            content[<span class="string">'title'</span>] = li.find(<span class="string">'a'</span>, attrs=&#123;<span class="string">"class"</span>: <span class="string">"j_th_tit"</span>&#125;).text.strip()  <span class="comment"># .strip()  翻译为中文</span></span><br><span class="line">            print(content[<span class="string">'title'</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 获取a标签的内部属性</span></span><br><span class="line">            content[<span class="string">'link'</span>] = <span class="string">"http://tieba.baidu.com/"</span> + li.find(<span class="string">'a'</span>, attrs=&#123;<span class="string">"class"</span>: <span class="string">"j_th_tit"</span>&#125;)[<span class="string">"href"</span>]</span><br><span class="line">            <span class="comment"># print("http://tieba.baidu.com/" + li.find('a', attrs=&#123;"class": "j_th_tit"&#125;)["href"])</span></span><br><span class="line"></span><br><span class="line">            content[<span class="string">'author'</span>] = li.find(<span class="string">'span'</span>, attrs=&#123;<span class="string">"class"</span>: <span class="string">'tb_icon_author'</span>&#125;).text.strip()</span><br><span class="line">            <span class="comment"># print(li.find('span', attrs=&#123;"class": 'tb_icon_author'&#125;).text.strip())</span></span><br><span class="line"></span><br><span class="line">            content[<span class="string">'authorLink'</span>] = <span class="string">"http://tieba.baidu.com"</span> + li.find(<span class="string">'a'</span>, attrs=&#123;<span class="string">"class"</span>: <span class="string">'frs-author-name j_user_card'</span>&#125;)[<span class="string">"href"</span>].replace(<span class="string">"amp;"</span>,<span class="string">""</span>)</span><br><span class="line">            <span class="comment"># print(li.find('span', attrs=&#123;"class": 'tb_icon_author'&#125;).text.strip())</span></span><br><span class="line"></span><br><span class="line">            content[<span class="string">'content'</span>] = li.find(<span class="string">'div'</span>, attrs=&#123;<span class="string">"class"</span>: <span class="string">'threadlist_abs threadlist_abs_onlyline'</span>&#125;).text.strip()</span><br><span class="line">            <span class="comment"># print(li.find('div', attrs=&#123;"class": 'threadlist_abs threadlist_abs_onlyline'&#125;).text.strip())</span></span><br><span class="line"></span><br><span class="line">            content[<span class="string">'responseNum'</span>] = li.find(<span class="string">'span'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'threadlist_rep_num center_text'</span>&#125;).text.strip()</span><br><span class="line">            <span class="comment"># print(li.find('span', attrs=&#123;'class': 'pull-right is_show_create_time'&#125;).text.strip())</span></span><br><span class="line"></span><br><span class="line">            content[<span class="string">'creatTime'</span>] = li.find(<span class="string">'span'</span>, attrs=&#123;<span class="string">"class"</span>: <span class="string">'pull-right is_show_create_time'</span>&#125;).text.strip()</span><br><span class="line">            <span class="comment"># print(li.find('span', attrs=&#123;'class': 'pull-right is_show_create_time'&#125;).text.strip())</span></span><br><span class="line"></span><br><span class="line">            content[<span class="string">'replyTime'</span>] = li.find(<span class="string">'span'</span>, attrs=&#123;<span class="string">"class"</span>: <span class="string">'threadlist_reply_date pull_right j_reply_data'</span>&#125;).text.strip()</span><br><span class="line">            <span class="comment"># print(li.find('span', attrs=&#123;'class': 'threadlist_reply_date pull_right j_reply_data'&#125;).text.strip())</span></span><br><span class="line"></span><br><span class="line">            author_html = getHtml(content[<span class="string">'authorLink'</span>])</span><br><span class="line">            <span class="comment"># 访问楼主的主页</span></span><br><span class="line"></span><br><span class="line">            author_bs = BeautifulSoup(author_html,<span class="string">'lxml'</span>)</span><br><span class="line">            <span class="comment"># 解析楼主信息</span></span><br><span class="line"></span><br><span class="line">            content[<span class="string">'sex'</span>] = re.compile(<span class="string">r"userinfo_sex userinfo_sex_(.*?)\""</span>, re.S).findall(str(author_bs))[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            content[<span class="string">'age'</span>] = re.compile(<span class="string">r"吧龄:(.*?)&lt;/"</span>, re.S).findall(str(author_bs))[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            content[<span class="string">'deliver'</span>] = re.compile(<span class="string">r"发贴:(.*?)&lt;/"</span>, re.S).findall(str(author_bs))[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            time.sleep(<span class="number">10</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 将字典加入到列表中</span></span><br><span class="line">            contents.append(content)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            print(<span class="string">'value maybe null'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 返回数据</span></span><br><span class="line">    <span class="keyword">return</span> contents</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">writeTxt</span><span class="params">(content,tiebaName,num)</span>:</span></span><br><span class="line">    <span class="comment">## 每页一个文件</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'&#123;&#125;-&#123;&#125;.json'</span>.format(tiebaName,num), <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">        json.dump(content, json_file, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(url, page, tiebaName)</span>:</span></span><br><span class="line">    url_list = []</span><br><span class="line">    <span class="comment"># 将所需要爬去的url放到列表中</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, page):</span><br><span class="line">        url_list.append(url + <span class="string">'&amp;pn='</span> + str(i * <span class="number">50</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(url_list)):</span><br><span class="line">        content = get_content(url_list[j], tiebaName)</span><br><span class="line">        writeTxt(content, tiebaName, j)</span><br><span class="line">        time.sleep(<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment">## 目标贴吧</span></span><br><span class="line">    tieba = [<span class="string">'中国传媒大学'</span>, <span class="string">'浙江传媒学院'</span>, <span class="string">'浙江传媒大学'</span>, <span class="string">'宅舞'</span>, <span class="string">'我们都爱自拍'</span>, <span class="string">'威海艺校'</span>, <span class="string">'网恋'</span>, <span class="string">'声控'</span>, <span class="string">'洛丽塔'</span>, <span class="string">'扩列'</span>, <span class="string">'滑稽'</span>, <span class="string">'韩国男团'</span>, <span class="string">'鬼畜'</span>, <span class="string">'二次元'</span>,</span><br><span class="line">             <span class="string">'处cp'</span>, <span class="string">'whoareyou学校2015'</span>, <span class="string">'QQ炫舞'</span>, <span class="string">'ost'</span>, <span class="string">'miss'</span>, <span class="string">'kpopstar'</span>, <span class="string">'JK'</span>, <span class="string">'05后早恋'</span>, <span class="string">'00后自拍'</span>, <span class="string">'00后早恋'</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tieba:</span><br><span class="line">        url = <span class="string">"https://tieba.baidu.com/f?kw=&#123;&#125;&amp;ie=utf-8"</span>.format(i)</span><br><span class="line">        <span class="comment">## 想要爬取的页数</span></span><br><span class="line">        page = <span class="number">60</span></span><br><span class="line">        main(url, page, i)</span><br></pre></td></tr></table></figure>

<h3 id="NLP及可视化部分"><a href="#NLP及可视化部分" class="headerlink" title="NLP及可视化部分"></a>NLP及可视化部分</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> wordcloud</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以宅舞为例，汇总每一页的file </span></span><br><span class="line"></span><br><span class="line">total = pd.DataFrame()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">60</span>):</span><br><span class="line">    filename = <span class="string">'宅舞-&#123;&#125;.json'</span>.format(i)</span><br><span class="line">    data=pd.read_json(filename,orient=<span class="string">'values'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">    total = total.append(data,ignore_index = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># 将标题内容合并为 string </span></span><br><span class="line"></span><br><span class="line">total_title_str = <span class="string">''</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(total)):</span><br><span class="line">    total_title_str = total_title_str + total[<span class="string">'title'</span>].loc[i]</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 进行分词，先分词后再清洗标点符号，好处是标点能够区分发帖人的语义，如果先将 string 清洗标点，可能会生成新的词。</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">statWordCount</span><span class="params">(words)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">## 进行分词</span></span><br><span class="line">    word_list = jieba.lcut(words)</span><br><span class="line">    word_list_new = []</span><br><span class="line">    <span class="comment">## 去除标点符合等信息</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> word_list:</span><br><span class="line">        word_etl = word.replace(<span class="string">"，"</span>, <span class="string">""</span>).replace(<span class="string">"【"</span>,<span class="string">""</span>).replace(<span class="string">"】"</span>,<span class="string">""</span>).replace(<span class="string">"！"</span>, <span class="string">""</span>).replace(<span class="string">"“"</span>, <span class="string">""</span>).replace(<span class="string">"”"</span>, <span class="string">""</span>).replace(<span class="string">"。"</span>, <span class="string">""</span>).replace(<span class="string">"？"</span>, <span class="string">""</span>).replace(<span class="string">"："</span>, <span class="string">""</span>).replace(<span class="string">"..."</span>, <span class="string">""</span>).replace(<span class="string">"、"</span>, <span class="string">""</span>).strip(<span class="string">' '</span>).strip(<span class="string">'\r\n'</span>)</span><br><span class="line">        word_list_new.append(word_etl)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 去除掉空字符</span></span><br><span class="line">    word_list_etl = list(filter(<span class="literal">None</span>,word_list_new))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## 词频统计</span></span><br><span class="line">    jieba_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> word_list_etl:</span><br><span class="line">        jieba_dict[key] = jieba_dict.get(key, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> word_list_etl,jieba_dict</span><br><span class="line"> </span><br><span class="line">jieba_result,jieba_dict = statWordCount(total_title_str)</span><br><span class="line"><span class="comment">## 生成DataFrame ，后续将词频转为csv，输出给需求方</span></span><br><span class="line">odb_df = pd.DataFrame.from_dict(jieba_dict,orient=<span class="string">'index'</span>)</span><br><span class="line">odb_df.columns = [<span class="string">"wordCount"</span>]</span><br><span class="line"><span class="comment">## 查看top20的word</span></span><br><span class="line">odb_df.sort_values(by=<span class="string">"wordCount"</span>,ascending= <span class="literal">False</span>).head(<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 词云部分，font_path路径为系统的字体路径</span></span><br><span class="line"></span><br><span class="line">word_str = <span class="string">" "</span>.join(jieba_result)</span><br><span class="line">w = wordcloud.WordCloud(width=<span class="number">1000</span>,</span><br><span class="line">                        height=<span class="number">700</span>,</span><br><span class="line">                        background_color=<span class="string">'white'</span>,</span><br><span class="line">                        font_path=<span class="string">'/System/Library/Fonts/PingFang.ttc'</span>)</span><br><span class="line"><span class="comment">## 生成词云，会自动过滤一些辅词 如：的，都，是</span></span><br><span class="line">w.generate(word_str)</span><br><span class="line"><span class="comment">## 保存为 PNG</span></span><br><span class="line">w.to_file(<span class="string">"young_word_clound.png"</span>)</span><br><span class="line"><span class="comment">## 打印到 jupyter</span></span><br><span class="line">wc_plc = plt.imread(<span class="string">"young_word_clound.png"</span>)</span><br><span class="line">plt.imshow(wc_plc)</span><br></pre></td></tr></table></figure>


        </div>
        <!-- Comments -->
        
    

    

    

    

    

    


      </div>
    </div>
  </div>
</article>



    
<!-- Footer -->
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <p class="copyright text-muted">
        &copy; 2016 -  2020
        <span class="with-love">
            <i class="fa fa-heart"></i>
        </span>
        <span class="author">zesen.he</span>

        

        
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
        <p class="visit-statistics">
            <span id="busuanzi_container_site_pv">
              本站总访问：<span id="busuanzi_value_site_pv"></span>次
              &nbsp;&nbsp;&nbsp;&nbsp;
            </span>
        <p>
        
      </div>
    </div>
  </div>
</footer>


    <!-- After Footer Scripts -->

<script src="/js/highlight.pack.js"></script>

<script>
  document.addEventListener("DOMContentLoaded", function(event) {
    var codeBlocks = Array.prototype.slice.call(document.getElementsByTagName('pre'))
    codeBlocks.forEach(function(block, index) {
      hljs.highlightBlock(block);
    });
  });
</script>



  </body>
</html>

